\section{Méthodes d'identification de mèmes dans un large volume de données}
\label{sec:id-meme}

Choisir un ensemble de mèmes cohérent est une des étapes difficiles de notre recherche. En effet, parmi les millions de messages de notre corpus et plus généralement dans la multitude des échanges quotidiens sur les réseaux sociaux, trouver une prise pour l{\textquoteright}étude n{\textquoteright}est pas une t\^ache évidente. Cette section présente les résultats obtenus lors de différentes expérimentations pour constituer des corpus de données représentatifs de mèmes dans un large volume de données.

\subsection[Constitution et collection d{\textquoteright}un corpus de messages]{Constitution et collection d{\textquoteright}un corpus de messages}
\label{sec:weiboscope}

La plupart des services de réseaux sociaux en ligne offrent un large accès à leur données. En effet, il s{\textquoteright}agit souvent de la fondation de leurs modèles d{\textquoteright}affaires basés sur la valorisation et la revente de ces données pour le marketing ciblé \citep{Ko2010}. Pour entrer en contact avec la base de données, les SNS mettent à disposition une API (\textit{Application Programming Interface}) qui permet à un programme ou une autre application web de se connecter au service pour demander et obtenir des données. L{\textquoteright}API est donc la première source d{\textquoteright}obtention de données depuis les SNS. Néanmoins, les données des réseaux sociaux sont soumises à d{\textquoteright}importants enjeux et contraintes tant commerciales, éthiques que politiques - dans le cas de la Chine notamment. Les conditions d{\textquoteright}utilisations techniques et légales (\textit{Terms of Uses}) de ces données sont également soumises à des changements fréquents, étroitement liées à l{\textquoteright}évolution commerciale et technologique de compagnies souvent très jeunes. Nous avons établi une liste des limitations et écueils pouvant \^etre rencontrés lors de l{\textquoteright}extraction et de l{\textquoteright}analyse de données des SNS :

\begin{description}
    
    \item[Compatibilité] 
    \hfill \\
    Une solution technologique devient facilement caduque lors de l{\textquoteright}évolution d{\textquoteright}une API (ex. la version  v1.0 de l'API de \textit{Twitter} n{\textquoteright}existe plus, ainsi le code doit \^etre réécrit pour la version 1.1).  
    
    \item [Disponibilité] 
    \hfill \\
    Chaque API répond à des formats et critères précis et possède ses propres limitations. Pour accéder à l{\textquoteright}API du moteur de recherche de \textit{Sina Weibo}, il faut s{\textquoteright}identifier auprès de la compagnie gr\^ace à une carte d{\textquoteright}identité et des paiements par requ\^ete sont exigés.

    \item[Limitations d{\textquoteright}usage] 
    \hfill \\
    Afin de limiter le trafic et conserver le contr\^ole sur les données distribuées, les SNS mettent en place des limitations d{\textquoteright}accès à leurs serveurs, notamment : requ\^etes par heure, requ\^etes par machine (basée sur l{\textquoteright}adresse IP), utilisateurs connectés. \textit{Twitter} limite à 150 requ\^etes API par heure pour un compte non-identifié, pouvant augmenter jusqu{\textquoteright}à 500 après authentification. Les données datant de plus de 7 jours sont payantes, reflétant la valeur d{\textquoteright}un accès en {\textquotedblleft}temps-réel{\textquotedblright}.
    
    \item[Légalité]
    \hfill \\
    Les données sont soumises aux conditions de propriété décrites légalement par la firme qui les publient \citep{Clifton2006}. Ces conditions sont susceptibles de changer. Ainsi, \textit{Twitter} a exigé en 2012 le retrait a posteriori de nombreux jeux de données publiés par des chercheurs depuis plusieurs années parfois \citep{McCreadie2012}. Actuellement, \textit{Twitter} précise notamment dans ses \textit{Terms of Use: {\textquotedblleft}You may not resyndicate or share Twitter content, including datasets of Tweet text and follow relationships{\textquotedblright} }\footnote{ Terms of use de Twitter, \url{https://dev.twitter.com/terms/api-terms}, consulté le 12 Mars 2013 à 17h08}.
    
    \item[Ethique]
    \hfill \\
    Vous pouvez extraire depuis une API des profils d{\textquoteright}utilisateurs contenant les informations qu{\textquoteright}ils ont auparavant publiées en ligne \citep{Felt2008}. En exposant les données personnelles des utilisateurs, le chercheur est responsable des risques qu{\textquoteright}il  peut faire courir à certains utilisateurs \citep{Rieder2005}.  
\end{description}

Afin de contourner les limitations de l{\textquoteright}API, la technique dite du \textit{{\textquotedblleft}scraping{\textquotedblright} }permet d{\textquoteright}obtenir des données. Un robot lit et sauvegarde des parties ou l{\textquoteright}intégralité de pages web. Les moteurs de recherche utilisent notamment cette technique pour l{\textquoteright}indexation des pages. Le scraping est également soumis à des limitations par les services web (blocage de l{\textquoteright}IP source) et se situe à la limite de la légalité, voire est explicitement interdit dans le cas de certains SNS \citep{Petschulat2010}.

Un programme appelé \textit{spider }ou \textit{crawler} est chargé d'obtenir et collecter des informations dans une base de données en effectuant des requ\^etes régulières à l{\textquoteright}API. Plusieurs approches existent dans les techniques d'échantillonnage de réseau social. La première, fondée sur des mots-clés, extrait les posts contenant des mots ou des hashtags particuliers. La seconde utilise l{\textquoteright}échantillonnage de graphe, collectant au fil des liens les conversations ou profils des utilisateurs. Classique des études statistiques, cet échantillonnage dit \textit{boule de neige {\textquotedblleft}élargit l{\textquoteright}échantillon en partant d{\textquoteright}un node original pour s{\textquoteright}éloigner vers ses voisins{\textquotedblright}} \citep{Rothenberg1995}. Ici, deux grandes catégories s{\textquoteright}opposent : les techniques transversales o\`u les nodes sont classifiés après avoir été visités et les {\textquotedblleft}walk{\textquotedblright} aléatoires o\`u l{\textquoteright}extension du graphe se fait de manière aléatoire \citep{Gjoka2011}.

Au cours du travail préparatoire de cette recherche, nous avons tout d{\textquoteright}abord expérimenté plusieurs algorithmes et outils de collection de données afin d{\textquoteright}en comparer les résultats. Une première approche d{\textquoteright}extraction par utilisateurs a été infructueuse car la sélection du groupe source (\textit{seeds}) ne permettait pas d{\textquoteright}obtenir des résultats cohérents\footnote{Code disponible : \url{https://github.com/sharismlab/Pyweibo}, consulté le 14 Mars à 5h32}. Par la suite, une autre approche de collecte de données via le développement d{\textquoteright}un plug-in pour le navigateur \textit{Google Chrome}\footnote{Code disponible : \url{https://github.com/sharismlab/battlefield}, consulté le 14 Mars à 5h12} nous a permis de tester et d{\textquoteright}apercevoir les limites de la collection de données par mots-clés. Cette étape nous a également montré l{\textquoteright}intér\^et que peut présenter une approche collaborative de la collection de données ou de seeds par un système collaboratif de ``curation'' pour réduire la masse de données obtenues à des éléments précis de contenus \citep{Ding2013}. Après de multiples tests et comparaisons d{\textquoteright}outils et de librairies, plusieurs difficultés majeures limitaient l{\textquoteright}obtention d{\textquoteright}une quantité de données suffisantes, notamment la nécessité de ressources assez importantes (en terme de développement et de disponibilité des machines), un temps d{\textquoteright}acquisition parfois très long et l{\textquoteright}exigence d{\textquoteright}une veille constante sur les SNS pour identifier un mème au bon moment (les tweets de Sina Weibo devenant indisponibles via l{\textquoteright}API au-delà de 7 jours). La première limite se situe bien s\^ur dans la capacité d{\textquoteright}une personne seule à mener à bien cette large t\^ache. 

Nous avons donc choisi de considérer les jeux de données déjà collectés et disponibles concernant \textit{Sina Weibo}. Une fois écartés les nombreux jeux tronqués, modifiés ou incomplets, nous avons pu obtenir plusieurs jeux de données provenant de recherches préalables dans le domaine particulier de l{\textquoteright}échantillonnage \citep{Ding2013} ou ayant servi de bases à des études précédentes \citep{Gao2012}. Finalement, nous avons identifié le jeu de données constitué lors du projet \textit{Weiboscope} de l{\textquoteright}Université de Hong Kong comme répondant à nos besoins en termes de dimensions (temps, nombre d{\textquoteright}utilisateurs observés), taille (nombre de tweets) et contenus (géo-localisation, présence des tweets censurés). Notre travail d{\textquoteright}analyse s{\textquoteright}appuie donc sur ce jeu de données collecté sur le service de microblog \textit{Sina Weibo} par le \textit{Journalism and Media Studies Centre} (JMSC) de l{\textquoteright}Université de Hong Kong lors de son projet \textit{Weiboscope}. Téléchargeable ouvertement, la publication de ce jeu de données a pour objectif de \textit{{\textquotedblleft}enables academic use of the data for better understanding of the social }\textit{media in China and making the Chinese media system more transparent.{\textquotedblright}}\footnote{Le jeu de données Weiboscope est disponible à l{\textquoteright}adresse : \url{http://147.8.142.179/datazip/}, consulté le 14 Mars 2014 à 17h21}\textit{ }Il s{\textquoteright}agit d{\textquoteright}un échantillonnage aléatoire de messages \textit{(random sampling)} effectué quotidiennement durant toute l{\textquoteright}année 2012 sur un panel d{\textquoteright}environ 350 000 utilisateurs ayant au moins 1000 followers \citep{Fu2013}. La totalité du jeu de données comprend 226,841,122 messages répartis sur 52 semaines, dont des messages ayant été supprimés par les utilisateurs eux-m\^emes ou par les administrateurs de Sina Weibo eux-m\^emes - parfois sur ordre du gouvernement chinois (\textit{ibid}, 2013).

\begin{table}[ht]
    \centering
    \small
    \begin{tabulary}{\textwidth}{c|C} 
        \toprule
        \textbf{Désignation}
        & \textbf{Description} \\
        \hline \\[-1.5ex]

        mid  &
        Unique pseudo message ID\\[2ex]
        retweeted\_status\_mid  &
        Pseudo message ID of the original message (Only available if the row of
        interest is a retweet)\\[2ex]
        Uid &
        Pseudo user ID\\[2ex]
        retweeted\_uid &
        Pseudo user ID of the original poster (Only available if the row of
        interest is a retweet)\\[2ex]
        Source &
        The application name of the client program\\[2ex]
        Image &
        With image? (1= Yes, 0=No)\\[2ex]
        text  &
        body of the message. Any address handle (@xxxx:) is replaced by either
        the pseudo user ID or ukn (unknown)\\[2ex]
        geo &
        GIS information. Please refer to the Sina Weibo API documentation:
        \url{http://goo.gl/Um8SS}\\[2ex]
        created\_at &
        Original posting time\\[2ex]
        deleted\_last\_seen &
        The last seen time before this message was missing from the user
        timeline\\[2ex]
        permission\_denied  &
        {\textquotesingle}permission denied{\textquotesingle} status is marked
        when the message was found missing in the timeline and the API return
        message was {\textquotesingle}permission denied{\textquotesingle} - See
        details in (Fu, Chan, Chau 2013)\\[2ex]
    \end{tabulary}
    \caption[Modèle de données du données Weiboscope]{Modèle de données du jeu de données \textit{Weiboscope}}
\end{table}

Ce jeu de données a été mis à disposition sous une forme anonymisée o\`u les identifiants des messages et des utilisateurs ont été remplacés par des pseudo-identifiants. La collection des données a été effectuée sur une série d{\textquoteright}utilisateurs (génération aléatoire d{\textquoteright}identifiants dont l{\textquoteright}existence est ensuite validée) pour donner \textit{{\textquotedblleft}une image représentative des usages et utilisateurs de Sina Weibo (...) dont les études auparavant limitées à des analyses non-aléatoires (...) se cantonnaient aux utilisateurs les plus populaires{\textquotedblright} }\citep{Fu2013}. Cet échantillon s{\textquoteright}attache à refléter les pratiques des utilisateurs {\textquotedblleft}lambda{\textquotedblright}. Ce jeu de données a déjà été partiellement étudié dans le but de comprendre la démographie des utilisateurs de Sina Weibo, leurs activités et les comportements pouvant permettre de prédire les réactions notamment de censure. La démographie des utilisateurs se compose de 55\% d{\textquoteright}hommes habitant principalement dans les grandes villes de Chine (Pékin, Canton, Shanghai). Une des découvertes importantes est le très faible taux de création originale de messages malgré une activité importante des utilisateurs, indiquant que l{\textquoteright}essentiel de l{\textquoteright}activité sur Sina Weibo se constitue de re-posts et de commentaires \citep{Fu2013}. Le jeu de données est accompagné des informations succintes de profil des utilisateurs dont le lieu, rempli par eux, sans néanmoins fournir leurs noms d{\textquoteright}utilisateurs véritables. Notre travail de recherche s{\textquoteright}articule autour d{\textquoteright}une nouvelle lecture de ce jeu de données unique.

\subsection[Détection algorithmique de mèmes]{Détection algorithmique de mèmes dans un large corpus de données}
\label{sec:protomemes}

Une fois l{\textquoteright}acquisition des données effectuée, il s{\textquoteright}agit désormais de savoir les analyser correctement pour y déceler les mèmes que nous souhaitons observer. Les travaux dans le domaine de la détection et l{\textquoteright}identification de mèmes dans les données de réseaux sociaux restent encore peu nombreux. Une des études pionnières est l{\textquoteright}outil \textit{MemeTracker }(devenu \textit{NIFTY}) con\c{c}u en 2009 par le \textit{SNA Project }de l{\textquoteright}Université de Stanford \citep{Leskovec2009}. Cet outil permet une étude sous forme de graphes de la diffusion de phrases dans un vaste corpus de texte mais n{\textquoteright}est pas adapté à la langue chinoise. La discussion sur la modélisation mathématiques des mèmes \citep{Ahmad2006, Nye2011} émane souvent de recherches en informatique cherchant à prendre en considération différents facteurs de diffusion lors de l{\textquoteright}analyse machine de données \citep{Zubiaga2011, Wang2011}, considérant parfois le mème comme un vecteur de modification du réseau lui-m\^eme \citep{Ienco2010}. Plus marginales, de rares études s{\textquoteright}intéressent aux dynamiques géographiques des mèmes \citep{Kamath2013}. Néanmoins, aucune de ces différentes approches ne permet d{\textquoteright}apporter une réponse technologique ou algorithmique satisfaisante pour l{\textquoteright}identification de mèmes Internet dans un ensemble de données issues des réseaux sociaux. Nous devons a priori déceler les motifs de diffusion particuliers formés par les mèmes dans ce vaste ensemble textuel. La dénomination \textit{machine learning }regroupe un ensemble d{\textquoteright}algorithmes qui permettent d{\textquoteright}explorer des jeux de données pour en extraire des représentations et y identifier des propriétés (\textit{features}) particulières. Basé sur les sciences statistiques, ces algorithmes font usage de mesures de similarité pour classifier les éléments d{\textquoteright}un corpus. Les catégories utilisées pour la classification peuvent \^etre définies au préalable par l{\textquoteright}utilisateur - c{\textquoteright}est le \textit{supervised learning} - ou inférées du jeu de données lui-m\^eme - dans le cas du \textit{unsupervised learning} \citep{Breiman2001}. La multitude d{\textquoteright}algorithmes de \textit{machine learning }disponibles pour la détection de \textit{clusters} au sein d{\textquoteright}un corpus textuel ou d{\textquoteright}un réseau social \citep{Nettleton2013, Robins2013} rend l{\textquoteright}identification d{\textquoteright}une solution difficile. De plus, les algorithmes utilisés traditionnellement pour l{\textquoteright}analyse de documents textuels (\textit{topic modeling, LSA}) se heurtent au caractère très disparate des corpus issus des réseaux sociaux (\textit{text sparsity issue) }faisant diminuer drastiquement leur efficacité \citep{Hong2010}. L'outil \textit{Sondy} développé par \cite{Guille2013a} nous a notamment permis d'explorer une partie du corpus à l'aide de différents algorithmes (fig. \ref{fig:sondy}). L'implémentation de l'analyse de langage chinoise et la faible efficacité des algorithmes à détecter des mèmes peu événementiels nous ont néanmoins amené à chercher des solutions plus spécialisées.


\begin{figure}
    \includegraphics[scale=.7]{figures/chap3/sondy.png}
    \caption[Captures d'écran de l'outil d'analyse de microblog Sondy]{Captures d'écran de l'outil d'analyse de microblog \textit{Sondy} d'après \url{http://mediamining.univ-lyon2.fr/people/guille/software.php} consulté le 15 Mars 2014 à 16:12}
    \label{fig:sondy}
\end{figure}

\cite{Ferrara2013} propose dans un papier intitulé {\textquotedblleft}\textit{Meme clustering in social media}{\textquotedblright} un algorithme utilisant la classification automatique non-supervisée pour détecter des mèmes dans un corpus de données de réseaux sociaux. Ce travail récent propose de tenir compte non seulement des textes et hashtags, mais aussi des liens et des modèles de diffusion pour identifier des groupes de messages intéressants et procéder au \textit{clustering }des mèmes. L{\textquoteright}algorithme s{\textquoteright}articule autour du concept de {\textquotedblleft}protomèmes{\textquotedblright}, représentant les éléments fondamentaux d{\textquoteright}un mème en cours de création \citep{Gabora1995}. Dans le contexte des médias sociaux, le protomème est défini par les entités (liens, hashtags...), mots-clés et séquences de conversation qui constituent le mème en devenir. En identifiant puis comparant les différents protomèmes présents dans chaque tweet, il est possible d{\textquoteright}y détecter des similarités et de deviner les mèmes en formation.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=5.8894in,height=1.2114in]{figures/chap3/chapitre3-img6.png}
    \caption{Algorithme de reconnaissance de mème (clustering) \citep{Ferrara2013}}
\end{figure}

Cet algorithme suppose donc d{\textquoteright}extraire dans un premier temps les éléments remarquables du corpus de tweets afin d{\textquoteright}établir des représentations de ces protomèmes contenant les éléments à comparer : \textit{phrases} (texte brut), \textit{mentions }(@, RT), \textit{hashtags} et \textit{urls}. Pour constituer ces jeux de protomèmes, nous utilisons le pattern \textit{map-reduce} qui permet de chercher et lister des éléments dans un vaste jeu de données \textit{(map)} avant de les regrouper dans une liste \textit{(reduce)}. Une fois ces protomèmes constitués, nous procédons à leurs comparaisons selon plusieurs critères :

\begin{itemize}
    \item \textit{Similarité de texte} : comparant le contenu textuel de chaque protomème 
    \item \textit{Similarité d{\textquoteright}utilisateurs} : comparant les utilisateurs contenus dans chaque protomème
    \item \textit{Similarités de tweet} : recherchant les tweets identiques dans différents protomèmes
    \item \textit{Similarité de diffusion} : considérant les références aux utilisateurs contenus dans chaque protomème
\end{itemize}

Ici nous utilisons la \textit{sémantique vectorielle} (Support Vector Machine) afin de comparer les éléments des protomèmes. Cette pratique ancienne de l{\textquoteright}algèbre linéaire appliquée à la science informatique \citep{Salton1975} permet de représenter des objets textuels (mots, identifiants, images...) sous une forme de vecteurs aisément comparable. Pour convertir le texte sous forme vectorielle, l{\textquoteright}algorithme classique Tf-idf (\textit{Term Frequency - Inverse Document Frequency}) est utilisé \citep{Soucy2005}. Les autres mesures de similarité sont la \textit{mesure cosine (}ou \textit{similarité cosinus) }des protomèmes convertis sous forme de vecteurs binaires. Une fois ces différentes valeurs de similarité calculées, nous utilisons les scalaires définis dans le papier de référence pour assigner des poids à chacun des vecteurs et les combiner en une seule valeur \citep{Ferrara2013}. Cette matrice de valeurs de similarité nous permet alors de définir les protomèmes les plus similaires et d{\textquoteright}identifier ainsi des \textit{clusters }dans les données correspondant aux mèmes.

Si cette approche offre des résultats probants sur de petits volumes (quelques centaines de tweets), la très grande demande en puissance de calcul et ressources mémoire nécessaires rendent le traitement d{\textquoteright}un jeu données plus vaste irréalisable. Les opérations de comparaison et le calcul de similarités sur de vastes volumes de données font cro\^itre très rapidement la quantité de calculs à effectuer. Le calcul du co\^ut d{\textquoteright}un algorithme se fait au travers des notions dites de domination, avec notamment le {\textquotedblleft}grand O{\textquotedblright} exprimé \textit{O(f(n)) }qui fait correspondre à la complexité d{\textquoteright}un algorithme une fonction \textit{f} de la quantité d{\textquoteright}information manipulée \textit{n. }Ainsi pour un algorithme courant de complexité \textit{O(n}\textit{\textsuperscript{2}}\textit{), }les ressources de calcul \textit{(computation)} et de mémoire (RAM ou stockage) nécessaires augmentent de manière exponentielle à chaque élément ajouté au corpus \textit{n}. L{\textquoteright}algorithme de {\textquotedblleft}meme clustering{\textquotedblright} décrit par Ferrara atteint donc un co\^ut exorbitant devant un large volume de données comme celui du jeu Weibosope. La limite physique de calcul est rapidement atteinte rendant impossible le franchissement du palier expérimental et la vérification des hypothèses de travail à une échelle suffisante. Ainsi, nous allons poursuivre ces expérimentations en nous intéressant désormais aux hashtags.

\subsection[Les hashtags ne sont pas des mèmes]{Les hashtags ne sont pas des mèmes}
\label{sec:hashtags}

Les \textit{hashtags }(en fran\c{c}ais {\textquotedblleft}mots-dièses{\textquotedblright}) sont utilisés dans l{\textquoteright}écriture sur les réseaux sociaux et se présentent dans \textit{Sina Weibo} sous la forme d{\textquoteright}un mot entouré de deux dièses - ex. \textit{\#mot-clé\#}. Marqueur particulier, le hashtag permet à un interlocuteur de procéder à une dénotation ou connotation du message original \citep{Romero2011} ou d{\textquoteright}affirmer son caractère événementiel (lors d{\textquoteright}un événement sportif, d{\textquoteright}une conférence, etc). Facile à identifier dans la masse des données en ligne, il permet de désigner un ensemble de contenus sous un m\^eme signe. Ainsi, il est un vecteur important permettant de collecter simplement une large somme d{\textquoteright}informations autour d{\textquoteright}un mème. La constitution d{\textquoteright}un corpus autour d{\textquoteright}un {\textquotedblleft}hashtag{\textquotedblright} présente néanmoins plusieurs limites. Premièrement, le mème est par définition un objet en mutation. Il est difficile de l{\textquoteright}identifier une bonne fois pour toute par un ou plusieurs mot-clés. De plus, le mème existe bien souvent sous la forme d{\textquoteright}images ou de vidéos qui ne sont pas nécessairement légendées ou référencées et donc peu accessibles à une recherche {\textquotedblleft}plein texte{\textquotedblright}. Une approche pour la recherche de mèmes ne peut \^etre entièrement textuelle et doit s{\textquoteright}intéresser aux autres forme de contenus web (notamment les liens). De plus, l{\textquoteright}ajout de hashtags dans les messages est un acte volontaire non systématique. Ainsi, si l{\textquoteright}identification de certains mèmes peut se faire gr\^ace à la recherche de hashtags, l{\textquoteright}ensemble des messages contenant des hashtags ne recouvre pas systématiquement un mème. Comme nous le verrons, les hashtags sont bien souvent de simples artefacts de campagne marketing en ligne.

Afin de procéder à l{\textquoteright}analyse des mèmes, nous avons donc indexé l{\textquoteright}ensemble des contenus du corpus Weiboscope contenant des hashtags sur toute l{\textquoteright}année 2012 (30 millions de messages sur un total de 200 millions environ). Dans un premier temps, l{\textquoteright}ensemble des messages contenant un ou des hashtags a été classifié pour obtenir des jeux de données cohérents par hashtags. L{\textquoteright}extraction des hashtags est effectuée gr\^ace à une expression régulière qui scanne le texte pour identifier et préserver uniquement les caractères situés entre les deux signes dièse (\#).


% Process Hashtags
\begin{figure}[htpb]
    \begin{algorithm}[H]
        \caption{Extract Hashtags from Message}
        \label{algo:message-graph}
        \begin{algorithmic}

            \Require{$m$ is a microblog text message}

            \Function{ExtractEntities}{$m$}
                
                \If{begins with \# and ends with \#}
                    \State $h\gets hashtag$
                    \State $text\gets u$ from 
                    % \State $H\gets->$h$
                \EndIf

            \EndFunction
        \end{algorithmic}
    \end{algorithm}
    \caption[Algorithme d'extraction d'entités d'après les messages]{
        \textit{Algorithme d'extraction d'entités d'après les messages}
        Les entités extraites des messages sont : les hashtags, les citations et les mots importants.

    }
\end{figure}

Sur un total d{\textquoteright}environ 30 millions de tweets contenant des hashtags, nous avons choisi de retenir seulement les hashtags contenus dans plus de 1000 messages et d{\textquoteright}ignorer les 1000 hashtags les plus utilisés. En effet, ils ne présentaient pas d{\textquoteright}intér\^et pour notre étude, étant la plupart du temps des noms de marque ou des mots-clés trop généraux (par exemple : {\textquotedblleft}bonne nuit{\textquotedblright}, {\textquotedblleft}nouvelles de sports{\textquotedblright}, {\textquotedblleft}photos de nourriture{\textquotedblright}, etc).

\begin{figure}[htpb]
    \centering
    
    \begin{tabular}{c|c|c|c}
        \textit{hashtags} & \textit{users} &  \textit{actions} & \textit{tweets} \\
        \hline\\ [-1ex]
        \zh{吴奇隆} & 201 & 13243 & 22349  \\
        \zh{一起到老} & 182 & 0 & 364  \\
        \zh{春运} & 92 & 13 & 256  \\
        \zh{轻松一刻} & 92 & 11 & 240  \\
        \zh{人品值分析} & 90 & 490 & 321  \\
        \zh{朝阳区} & 88 & 49 & 165  \\
        \zh{理性态小度} & 87 & 0 & 329  \\
        \zh{美图GIF} & 87 & 101 & 404  \\
        \zh{我正在听} & 86 & 6 & 330  \\
        \zh{微盘签到} & 84 & 304 & 305  \\
        \zh{2012来了} & 83 & 206 & 309  \\
        \zh{中级达人} & 83 & 0 & 159  \\
        \zh{分享} & 82 & 87 & 563  \\
        \zh{星座} & 82 & 5 & 195  \\
    \end{tabular}

    \caption[Hashtags les plus utilisés pendant la 1ère semaine de 2012]{\textit{Hashtags les plus utilisés pendant la 1ère semaine de 2012} - Les résultats concernant la 1ère semaine de l{\textquoteright}année 2012 donnent un aper\c{c}u du volume analysé : 5,044,331 tweets, 398 392 utilisateurs uniques cités (dans un total de 2 115 544 mentions), 264 651 urls uniques (pour un total de 426 914) et 44 382 hashtags uniques (pour un total de 244285)}.
    \label{fig:most-hashtags}
\end{figure}


Notre étude vise à analyser les dynamiques conversationnelles et nous devons donc déterminer les plus adéquats parmi des hashtags de nature souvent très différentes. Pour ce faire, nous avons sélectionné pour chaque hashtag deux mesures significatives : premièrement, le volume de messages ; deuxièmement, la quantité d{\textquoteright}échanges et d{\textquoteright}interactions effectives entre les utilisateurs (commentaires, retweets, etc.). Ces deux mesures nous permettent de nous assurer que 1) nous possédons une quantité suffisante de messages pour mener à bien l{\textquoteright}étude et que 2) la discussion a bien eu lieu et qu{\textquoteright}il ne s{\textquoteright}agit pas de messages redondants ou non reliés entre eux. Nous avons choisi d{\textquoteright}ignorer les échanges dominés à plus de 80\% par le m\^eme utilisateur pour éviter la pollution de l{\textquoteright}étude par l{\textquoteright}activité de robots. Le graphe \ref{fig:distrib-hashtags} permet d{\textquoteright}observer la distribution de 429 hashtags possédant tous plus de 1000 tweets et 1000 échanges : l{\textquoteright}axe vertical représente la quantité d{\textquoteright}actions (échanges) et l{\textquoteright}axe horizontal le volume des conversations. Tout en bas du graphe se trouvent donc les hashtags ayant été les moins discutés, avec en haut ceux aux conversations les plus intenses. La taille des points illustre le volume de messages et la couleur la quantité de conversations.


\begin{figure}[htpb]
    \centering
    \includegraphics[width=6.0114in,height=3.2114in]{figures/chap3/chapitre3-img8.png}
    \caption{Distribution des 429 hashtags les plus discutés}
    \label{fig:distrib-hashtags}
\end{figure}


En procédant à l{\textquoteright}étiquetage des hashtags les plus actifs durant l{\textquoteright}année 2012 sur \textit{Sina Weibo} (figure \ref{fig:distrib-hashtags}), nous constatons que la plupart sont associés à des activités commerciales, de loisirs ou de divertissement. Ici nous observons que les usages majoritaires du réseau social \textit{Sina Weibo} correspondent pour la plupart à ceux d{\textquoteright}autres mass-médias plus traditionnels de par le monde. Le commerce en ligne occupe notamment une place proéminente. La marque de téléphonie mobile chinoise \textit{Xiaomi} est abondamment citée, reflétant son importance croissante dans le marché chinois et surtout sa stratégie commerciale qui cible abondamment les réseaux sociaux avec de nombreux hashtags très discutés (notamment \textit{{\textquotedblleft}Fans de Xiaomi{\textquotedblright} } \#\zh{小米粉丝}\# ). Également, de nombreuses campagnes promotionnelles d{\textquoteright}ouverture ou d{\textquoteright}anniversaire de magasins ont réussi à se hisser dans le jeu de t\^ete des hashtags les plus discutés. Dernier-nés des radio-crochets ou chanteurs reconnus, les stars de la télévision et de la chanson sont aussi présentes dans le peloton de t\^etes des discussions sur \textit{Sina Weibo}. Le célèbre chanteur Han Geng notamment compte près d{\textquoteright}une dizaine de hashtags le concernant parmi les 500 les plus discutés (\textit{{\textquotedblleft}Han Geng fait une pub pour Nokia{\textquotedblright}, {\textquotedblleft}Han Geng va en Italie{\textquotedblright}, {\textquotedblleft}Han Geng fait une pub Pepsi{\textquotedblright}, {\textquotedblleft}Han Geng refuse une interview{\textquotedblright}, etc.)} Ici encore, le réseau social agit comme le prolongement des mass média traditionnels, élément-clé des nouvelles stratégies de publicités en ligne, parfois particulièrement agressives comme dans le cas de Han Geng. Les contenus de la télévision sont largement relayés et discutés, notamment les séries télévisuelles. Le cinéma est aussi représenté. Plus grand succès commercial du box-office chinois, le film comique \textit{Lost in Thailand} sorti en Décembre 2012 dépeint les aventures d{\textquoteright}un chinois en vacances en Thailande. Sa popularité se reflète dans l{\textquoteright}importance au sein des discussions en ligne. Les tendances des ventes du livre sont également au rendez-vous avec de nombreux best-seller sur {\textquotedblleft}l{\textquoteright}amélioration de soi{\textquotedblright} ou la {\textquotedblleft}réussite économique{\textquotedblright}\textit{. }Ce type de hashtags ne se limite pas au support web mais s{\textquoteright}origine directement dans d{\textquoteright}autres médias plus traditionnels. Le gouvernement lui-m\^eme utilise \textit{Sina Weibo} pour faire passer ses messages avec un hashtag \textit{{\textquotedblleft}information officielle{\textquotedblright} }utilisé notamment pour des démentis publics ou droit de réponse par l{\textquoteright}entreprise \textit{Sina}, propriétaire du service. \'Egalement outil de conversation, le microblog héberge les discussions de la vie de tous les jours. La situation routière et les bouchons dans chaque ville sont un des grands sujets de discussions. Ce sont dans ces échanges quotidiens que se cristallisent plus particulièrement les enjeux politiques et médiatiques des réseaux sociaux. Nouveau café du commerce, les commentaires sur les faits divers et l{\textquoteright}actualité mettent souvent à jour les dysfonctionnements de systèmes politiques, urbains ou légaux. 

Il est intéressant néanmoins de noter que parmi les hashtags les plus discutés, les phénomènes de suppression de contenus par les administrateurs (censure) restent très marginaux. Le \textit{China Digital Times} de UC Berkeley maintient une liste des mots interdits sur \textit{Sina Weibo} depuis plusieurs années \citep{Ng2013}. En comparant cette liste de mots censurés à celle des hashtags, nous avons pu voir qu{\textquoteright}aucun des 3000 hashtags les plus utilisés en 2012 n{\textquoteright}a été soumis a une interdiction m\^eme temporaire sur \textit{Sina Weibo}. Les hashtags les plus sujets à la censure ne sont pas en lien avec des domaines politiques ou des sujets sensibles, mais plut\^ot avec des contenus à caractère pornographique (interdits en Chine). Reflétant les usages majoritaires (commerce, loisirs, etc.), les hashtags véhiculent des contenus souvent moins controversés et les {\textquotedblleft}mots censurés{\textquotedblright} sont plus à m\^eme d{\textquoteright}appara\^itre dans des discussions informelles.

Ces observations nous amènent donc à interroger la pertinence du hashtag comme représentant des mèmes Internet. En effet, la plupart des hashtags semblent être le reflet de campagnes de communication dont la diffusion très planifiée ne laisse que peu de place à des interactions spontanées. L'aspect stratégique du hashtag en fait un artefact de campagnes de diffusion, définition seulement partielle et insatisfaisante du mème Internet.  Ainsi, nous préférons écarter l'usage de ce marqueur pour observer les mèmes.

\bigskip

Les expérimentations présentées précédemment nous ont permis de réunir, tester et sélectionner un ensemble d'outils et de méthodes parmi ceux disponibles. Ces diverses tentatives ont notamment montré les limites de chacune des approches. \'A la lumière de ces expérimentations et réflexions, nous allons maintenant présenter les solutions retenues pour la poursuite de cette étude.
